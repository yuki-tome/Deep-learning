{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 2.\n",
    "\n",
    "## Student Name/ID: Yuki Tome/m5271046"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MLP using PyTorch `nn` Modules\n",
    "\n",
    "The aim of this assignment is to create similar structure as in exercise 2 using PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define default data type and device for tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)  # we set up a seed so that your output matches ours although the initialization is random.\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for data loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('/datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('/datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will use the same image dataset that has two classes cat and non-cat. Every image is represented with numpy array of shape \\[num_pixels, num_pixels, 3\\] (3 is number of RGB channels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "\n",
    "# Figuring out the dimensions and shapes of the problem\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "print(\"Number of training examples: m_train = \" + str(m_train))\n",
    "print(\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print(\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print(\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print(\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_y shape: \" + str(train_set_y_orig.shape))\n",
    "print(\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_y shape: \" + str(test_set_y_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to reshape these images in a numpy-array of shape (num_samples, num_pixels $*$ num_pixels $*$ 3) and labels in a numpy array of shape (num_samples, num_labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1)\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1) \n",
    "\n",
    "# Reshape labels dataset\n",
    "train_set_y = train_set_y_orig.T\n",
    "test_set_y = test_set_y_orig.T\n",
    "\n",
    "# \"Standardize\" the data\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can create PyTorch Tensors to use it further to train and test our PyTorch model (same as in part 2).\n",
    "\n",
    "**Hint**\n",
    "Here we use previously defined `dtype`. And we set `requires_grad` parameter to False since it's only data holder.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(train_set_x, dtype=dtype, requires_grad=False)\n",
    "Y_train = torch.tensor(train_set_y, dtype=dtype, requires_grad=False)\n",
    "X_test = torch.tensor(test_set_x, dtype=dtype, requires_grad=False)\n",
    "Y_test = torch.tensor(test_set_y, dtype=dtype, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will follow similar methodology as in parts 1 and 2.\n",
    "\n",
    "**The general methodology to build a Neural Network using PyTorch is to:**\n",
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "    - Obtain input and output layer sizes\n",
    "    - **Problem 1:** Define type of layers and activations you want to use in your model, which will be used for\n",
    "        forward propagation.\n",
    "2. Initialize the model's parameters\n",
    "3. **Problem 2:** Define loss function\n",
    "4. Training loop:\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "And finally implement `train_model()` by using previous functions in the right order. This will be your **Problem 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Defining sizes of input and output layers ###\n",
    "\n",
    "First, we define sizes of input and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (input size, number of examples)\n",
    "    Y -- labels tensor of shape (output size, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.size(1)\n",
    "    n_y = Y.size(1)\n",
    "\n",
    "    return n_x, n_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1 ##\n",
    "### Step 1.2: Defining structure of the neural network ###\n",
    "\n",
    "Implement function `create_model`. It should have the same functionality as `forward_propagation` function in parts 1 and 2. Your model should consists of:\n",
    "    - one hidden layer with tanh activation function,\n",
    "    - one output layer with sigmoid activation function\n",
    "\n",
    "**Tips**\n",
    "\n",
    "You can define your model as a sequence of Modules (layers, activations, etc.) using [sequential module](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential), which contains other Modules and applies them in sequence to produce its output.\n",
    "\n",
    "In parts 1 and 2 of this exercise, we used Dense or [Linear](https://pytorch.org/docs/master/nn.html#linear-layers) layers as hidden and output. You can check in [documentation](https://pytorch.org/docs/master/nn.html#non-linear-activations-weighted-sum-nonlinearity) which activation functions are available in PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    Returns:\n",
    "    model -- model consisting of one hidden layer and one output layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here !!!\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_x, n_h),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(n_h, n_y),\n",
    "        torch.nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure is:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Check implementation\n",
    "model_tmp = create_model(10, 5, 1)\n",
    "print('Model structure is:')\n",
    "print(model_tmp)\n",
    "# Expected output: Sequential( (0):..., (1):..., (2):..., (3):...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Initialize parameters of the model ###\n",
    "When you create a layer using PyTorch Module, weights and biases are created and initialized automatically. However, default initialization algorithm might be not optimal for your task. \n",
    "1. Initialize weight matrices using orthogonal initialization scheme from [torch.nn.init](https://pytorch.org/docs/master/nn.html#torch-nn-init).\n",
    "2. Initialize biases with zeros.\n",
    "    \n",
    "**Hint:** You can use `model.named_parameters()` function to access your model parameters and their respective names, so that you can distinguish weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(model):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.find('weight') != -1:\n",
    "            torch.nn.init.orthogonal_(param)\n",
    "        elif name.find('bias') != -1:\n",
    "            torch.nn.init.constant_(param, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight 0.768763542175293\n",
      "0.bias 0.0\n",
      "2.weight 0.6325703859329224\n",
      "2.bias 0.0\n"
     ]
    }
   ],
   "source": [
    "initialize_parameters(model_tmp)\n",
    "\n",
    "for name, param in model_tmp.named_parameters():\n",
    "    print(name, param.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 ##\n",
    "\n",
    "### Step 3: Define loss function ###\n",
    "\n",
    "Here, you need to declare loss function, that will be used later in `train_model()`. You should also use binary cross-entropy as in parts 1 and 2, but you should define it using built-in PyTorch loss function.\n",
    "\n",
    "**Tips** Check in documentation, which [loss functions](https://pytorch.org/docs/master/nn.html#loss-functions) are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cost_function():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    loss - binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Your code here !!!\n",
    "    return torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4.1** ###\n",
    "\n",
    "Same as in part 2, we will use automatic differentiation to automate the computation of backward passes in neural networks. So you will define it in your training loop.\n",
    "\n",
    "The `autograd` package in PyTorch provides exactly this functionality. When using `autograd`, the forward pass of your network will define a computational graph. Backpropagating through this graph then allows you to easily compute gradients. See documentation for more details: https://pytorch.org/docs/master/autograd.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4.2 ###\n",
    "\n",
    "Implement the manual update rule using gradient descent. \n",
    "\n",
    "**General gradient descent rule**: \n",
    "\n",
    "$ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "  You can access gradients using `parameter.grad`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "\n",
    "    Arguments:\n",
    "    model -- python dictionary containing your parameters \n",
    "    \"\"\"\n",
    "    # Since we use manual update of parameters, we need to wrap in torch.no_grad()\n",
    "    # because all parameters have requires_grad=True, but we don't need to track this.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            # Your code here !!!\n",
    "            param -= learning_rate*param.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Use model created by `create_model()` to predict by building `predict()` function. \n",
    "\n",
    "As you can recall from step 1.2, your model is a sequential Module. Module objects override the `__call__` operator so you can call them like functions. When doing so you pass a Tensor of input data to the Module and it produces a Tensor of output data.\n",
    "\n",
    "**Hint**: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    \n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```. Remember that it will return `LongTensor`, so you need to cast it back to `FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    \"\"\"\n",
    "    Using network output, predicts a class for each example in X\n",
    "\n",
    "    Arguments:\n",
    "    model -- trained model\n",
    "    X -- network output data of size (m, 1)\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (non-cat: 0 / cat: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    Y_predicted = model(X)\n",
    "    predictions = (Y_predicted > 0.5).float()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 ##\n",
    "\n",
    "### Integrate all steps to train your model ###\n",
    "\n",
    "Implement full training process of your neural network model in `train_model()` function.\n",
    "\n",
    "**Instructions**: The neural network model has to use previous functions in the right order.\n",
    "\n",
    "**Hint:** Check out [Module functions](https://pytorch.org/docs/master/nn.html#torch.nn.Module) to find one for zeroing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_test, Y_test, n_h, num_iterations=10000,\n",
    "                learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- number of iterations in gradient descent loop\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- if True, print the cost every 200 iterations\n",
    "\n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    n_x, n_y = layer_sizes(X_train, Y_train)\n",
    "\n",
    "    # Replace None with your own code !!!\n",
    "\n",
    "    # Create model\n",
    "    model = create_model(n_x, n_h, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(model)\n",
    "\n",
    "    # Cost function\n",
    "    cost_fn = create_cost_function()\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: compute model outputs by passing input data to the model.\n",
    "        A2 = model(X_train)\n",
    "\n",
    "        # Cost function. Inputs: predicted and true values. Outputs: \"cost\".\n",
    "        cost = cost_fn(A2,Y_train)\n",
    "\n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost.item()))\n",
    "\n",
    "        # Zero the gradients before running the backward pass. See hint in problem description\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Backpropagation. Compute gradient of the cost function with respect to all the \n",
    "        # learnable parameters of the model. Use autograd to compute the backward pass.\n",
    "        # Same as in part 2 of Exercise 2.\n",
    "        cost.backward()\n",
    "\n",
    "        # Gradient descent parameter update.\n",
    "        parameters = update_parameters(model, learning_rate)\n",
    "\n",
    "    d = {\"model\": model,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.775971\n",
      "Cost after iteration 100: 0.523547\n",
      "Cost after iteration 200: 0.496528\n",
      "Cost after iteration 300: 0.455826\n",
      "Cost after iteration 400: 0.493387\n",
      "Cost after iteration 500: 0.252120\n",
      "Cost after iteration 600: 0.176568\n",
      "Cost after iteration 700: 0.198569\n",
      "Cost after iteration 800: 0.075340\n",
      "Cost after iteration 900: 0.048445\n",
      "Cost after iteration 1000: 0.032887\n",
      "Cost after iteration 1100: 0.025090\n",
      "Cost after iteration 1200: 0.020666\n",
      "Cost after iteration 1300: 0.017462\n",
      "Cost after iteration 1400: 0.015119\n",
      "Cost after iteration 1500: 0.012856\n",
      "Cost after iteration 1600: 0.011347\n",
      "Cost after iteration 1700: 0.010207\n",
      "Cost after iteration 1800: 0.009280\n",
      "Cost after iteration 1900: 0.008502\n",
      "train accuracy: 100.0 %\n",
      "test accuracy: 68.0 %\n"
     ]
    }
   ],
   "source": [
    "d = train_model(X_train, Y_train, X_test, Y_test, 15,\n",
    "                num_iterations=2000, learning_rate=0.05, print_cost=True)\n",
    "\n",
    "# Predict test/train set examples\n",
    "Y_prediction_test = predict(d[\"model\"], X_test)\n",
    "Y_prediction_train = predict(d[\"model\"], X_train)\n",
    "# Print train/test Errors\n",
    "print(\"train accuracy: {} %\".format(100 - torch.mean(torch.abs(Y_prediction_train - Y_train)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - torch.mean(torch.abs(Y_prediction_test - Y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional #\n",
    "\n",
    "1. Implement automatic update of model parameters using [torch.optim](https://pytorch.org/docs/master/optim.html) package.\n",
    "2. Add layers to your model and change activation functions.\n",
    "3. Add regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
