{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 \n",
    "\n",
    "\n",
    "## Student Name/ID: ____\n",
    "\n",
    "---\n",
    "\n",
    "## Improving training of sequence-to-sequence network. ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will build an improved version of Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\"). \n",
    "\n",
    "As you know from the last lecture, a Sequence to Sequence network, or seq2seq network, or Encoder Decoder network, is a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence. See lecture slides 32~ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need several helper functions to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "\n",
    "Faker.seed(12345)\n",
    "random.seed(12345)\n",
    "fake = Faker()\n",
    "\n",
    "# Define format of the data we would like to generate\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY',\n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "LOCALES = ['en_US']\n",
    "\n",
    "\n",
    "def load_date():\n",
    "    \"\"\"\n",
    "        Loads some fake dates \n",
    "        :returns: tuple containing human readable string, machine readable string, and date object\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS), locale='en_US') # locale=random.choice(LOCALES))\n",
    "        human_readable = human_readable.lower()\n",
    "        human_readable = human_readable.replace(',', '')\n",
    "        machine_readable = dt.isoformat()\n",
    "\n",
    "    except AttributeError:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt\n",
    "\n",
    "\n",
    "def load_dataset(m):\n",
    "    \"\"\"\n",
    "        Loads a dataset with m examples and vocabularies\n",
    "        :m: the number of examples to generate\n",
    "    \"\"\"\n",
    "\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "\n",
    "    for i in tqdm(range(m)):\n",
    "        h, m, _ = load_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "\n",
    "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'],\n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
    "    machine = {v: k for k, v in inv_machine.items()}\n",
    "\n",
    "    return dataset, human, machine, inv_machine\n",
    "\n",
    "\n",
    "def string_to_int(string, length, vocab):\n",
    "    \"\"\"\n",
    "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
    "    input string's characters in the \"vocab\"\n",
    "\n",
    "    Arguments:\n",
    "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
    "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
    "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
    "\n",
    "    Returns:\n",
    "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    # make lower to standardize\n",
    "    string = string.lower()\n",
    "    string = string.replace(',', '')\n",
    "\n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "\n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "\n",
    "    if len(string) < length:\n",
    "        rep += [vocab['<pad>']] * (length - len(string))\n",
    "\n",
    "    return rep\n",
    "\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(nbatch, bsz, data.size(1)).contiguous()\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty, batch_size=40):\n",
    "\n",
    "    X, Y = zip(*dataset)\n",
    "\n",
    "    X = torch.tensor(np.array([string_to_int(i, Tx, human_vocab) for i in X]), dtype=torch.long, device=device)\n",
    "    Y = torch.tensor(np.array([string_to_int(t, Ty, machine_vocab) for t in Y]), dtype=torch.long, device=device)\n",
    "\n",
    "    return batchify(X, batch_size), batchify(Y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, human_vocab, machine_vocab, inv_machine = load_dataset(10000)\n",
    "print(random.choice(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess the data and map the raw text data into the index values. We will also use Tx=30 (which we assume is the maximum length of the human readable date; if we get a longer input, we'd have to truncate it) and Ty=10 (since \"YYYY-MM-DD\" is 10 characters long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word. This part of your model will be the same as in the Exercise 5 part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, input.size(0), -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder with attention\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "## Problem 1:\n",
    "\n",
    "Insert batch normalization layer before GRU layer. \n",
    "\n",
    "Check [documentation for batch normalization layers](https://pytorch.org/docs/stable/nn.html#normalization-layers). \n",
    "\n",
    "**Hint**\n",
    "Be carefull with layer sizes. RNN layers expects tensors of size (sequence_length x batch_size x input_size), in this case it's ( 1 x 40 x 256 ). While `BatchNorm1D` layer expects tensor of size (batch_size x input_size). Since sequence length in the decoder part is always 1, you only need to remove first dimension before batch normalization and add it back after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p, max_length=Tx):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        self.attn = torch.nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = torch.nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(self.dropout_p)\n",
    "\n",
    "        # !!! Your code here:\n",
    "        self.batch_norm = \n",
    "\n",
    "        self.gru = torch.nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, input.size(0), -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[:, 0, :]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = torch.nn.functional.relu(output)\n",
    "\n",
    "        # !!! Your code here:\n",
    "        output = \n",
    "\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = torch.nn.functional.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "First let's make some helper functions to plot losses while training and to print time elapsed and estimated time remaining given the current time and progress %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import time\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train step** \n",
    "First, we write `train_step()` function to perform one training step over sentence pair. In the Exercise 5 part 2, we used model output as decoder input for next time step. There's a [\"Scheduled sampling\"](https://arxiv.org/abs/1506.03099) concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. Using scheduled sampling causes model to converge faster.\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "Implement random sampling of the decoder input.\n",
    "- Get sampling probability and decide whether to use sampling or not.\n",
    "- Feed the target as the next input\n",
    "\n",
    "Because of the freedom PyTorch’s autograd gives us, we can randomly choose to use sampling or not with a simple if statement. Turn `sampling_ratio` up to use more or less of it.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "Use `random.random()` to get sampling probability and compare its value with `sampling_ratio` to decide whether to use target or decoder's guess as decoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          criterion, sampling_ratio=0.5, max_length=Tx):\n",
    "    encoder_hidden = encoder.initHidden(40)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(1)\n",
    "    target_length = target_tensor.size(1)\n",
    "\n",
    "    encoder_outputs = torch.zeros(40, max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[:, ei], encoder_hidden)\n",
    "        encoder_outputs[:, ei, :] = encoder_output\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([len(machine_vocab)]*40), dtype=torch.long, device=device)\n",
    "    decoder_input = decoder_input.view(40, -1)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # !!! Your code here:\n",
    "    if   :\n",
    "        use_sampling = True\n",
    "    else:\n",
    "        use_sampling = False\n",
    "\n",
    "    if use_sampling:\n",
    "        # Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[:, di])\n",
    "\n",
    "            # !!! Your code here:\n",
    "            decoder_input =\n",
    "\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            loss += criterion(decoder_output, target_tensor[:, di])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch training\n",
    "\n",
    "## Problem 3\n",
    "\n",
    "Use [`Adam`](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) optimizer to train encoder and decoder. You need to adjust your learning rate. It's most sensitive hyper-parameter and might differ a lot depending on optimizer and other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_epochs, print_every=1000, plot_every=100, learning_rate=0.005):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    n_iters = n_epochs * X.size(0)\n",
    "\n",
    "    # !!! Your code here:\n",
    "    encoder_optimizer =\n",
    "    decoder_optimizer =\n",
    "\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    for ep in range(n_epochs):\n",
    "        for b_id in range(X.size(0)):\n",
    "            input_tensor = X[b_id]\n",
    "            target_tensor = Y[b_id]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer,\n",
    "                         criterion, sampling_ratio=0.5)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if (b_id % print_every == 0) and b_id != 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, (b_id + ep*X.size(0)) / n_iters),\n",
    "                      (b_id + ep*X.size(0)), (b_id + ep*X.size(0)) / n_iters * 100, print_loss_avg))\n",
    "                evaluateRandomly(encoder, decoder, 1)\n",
    "                # Set training mode for encoder and decoder\n",
    "                encoder.train()\n",
    "                decoder.train()\n",
    "\n",
    "            if (b_id % plot_every == 0) and b_id != 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so we simply feed the decoder’s predictions back to itself for each step. Every time it predicts a word we add it to the output string, and if it predicts the EOS token we stop there. We also store the decoder’s attention outputs for display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, max_length=Tx):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        input_length = input_tensor.size(1)\n",
    "        encoder_hidden = encoder.initHidden(1)\n",
    "\n",
    "        encoder_outputs = torch.zeros(1, max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                input_tensor[:, ei], encoder_hidden)\n",
    "            encoder_outputs[:, ei, :] = encoder_output\n",
    "\n",
    "        decoder_input = torch.tensor(np.array([len(machine_vocab)]*1), dtype=torch.long, device=device)\n",
    "        decoder_input = decoder_input.view(1, -1)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(Ty):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoded_words.append(inv_machine[topi.item()])\n",
    "\n",
    "            decoder_input = topi.detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    # This disables Dropout and BatchNormalization operation during the test.\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    for i in range(n):\n",
    "        pair = random.choice(dataset)\n",
    "        idx = [dataset.index(pair) // 40, dataset.index(pair) % 40]\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder,\n",
    "                                            X[dataset.index(pair) // 40, dataset.index(pair) % 40])\n",
    "        output_sentence = ''.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but it makes it easier to run multiple experiments) we can actually initialize a network and start training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(len(human_vocab), hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, len(machine_vocab)+1, dropout_p=0.2).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5, print_every=50, plot_every=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
